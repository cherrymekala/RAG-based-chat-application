{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRsdruwR62zm",
        "outputId": "8a1ce843-48f7-4adb-e9d3-ec5eb9b8435d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.50.2-py3-none-any.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.0/383.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pinecone-plugin-interface, jiter, h11, pinecone-plugin-inference, httpcore, pinecone-client, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.50.2 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install openai pinecone-client\n",
        "\n",
        "import pinecone\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Configuring Pinecone with our API key and environment\n",
        "pc = Pinecone(api_key='YOUR_PINECONE_API_KEY')\n",
        "\n",
        "PINECONE_INDEX_NAME = 'chat-history'\n",
        "\n",
        "if PINECONE_INDEX_NAME not in pc.list_indexes():\n",
        "    pc.create_index(\n",
        "        name=PINECONE_INDEX_NAME,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )\n",
        "\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "# Configuring Gemini with it's API key and generation configuration\n",
        "genai.configure(api_key='YOUR_GEMINI_API_KEY')\n",
        "\n",
        "# Generation configuration\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "# GenerativeModel instance\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")"
      ],
      "metadata": {
        "id": "hIRODkiJ7c0V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [\n",
        "  \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
        "  \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
        "  \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
        "  \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
        "  \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
        "  \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
        "  \"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
        "  \"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
        "  \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
        "  \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
        "  \"11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.\",\n",
        "  \"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\",\n",
        "  \"13: User: Nice! Any good board games? | Bot: Settlers of Catan and Ticket to Ride are both excellent choices.\",\n",
        "  \"14: User: I'll try them out. Any movie recommendations? | Bot: 'Inception' and 'The Matrix' are must-watch movies.\",\n",
        "  \"15: User: I love those movies! Any TV shows? | Bot: 'Breaking Bad' and 'Stranger Things' are very popular.\",\n",
        "  \"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\",\n",
        "  \"17: User: Thanks! What are some good travel destinations? | Bot: Paris, Tokyo, and Bali are amazing travel spots.\",\n",
        "  \"18: User: I'll add them to my list. Any packing tips? | Bot: Roll your clothes to save space and use packing cubes.\",\n",
        "  \"19: User: That's helpful! What about travel insurance? | Bot: Always get travel insurance for safety and peace of mind.\",\n",
        "  \"20: User: Thanks for the tips! Any last advice? | Bot: Just enjoy your journey and make the most out of your experiences.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "0D5I0BzRQv0J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_embeddings_to_pinecone(history, index_name='chat-history'):\n",
        "\n",
        "    if index_name not in pc.list_indexes():\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=768,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(\n",
        "                cloud='aws',\n",
        "                region='us-east-1'\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Embedding conversation history using Gemini\n",
        "    embeddings = genai.embed_content(model=\"models/text-embedding-004\", content=history)['embedding']\n",
        "\n",
        "    vectors=[]\n",
        "  # Add our embeddings to Pinecone\n",
        "    for i, embedding in enumerate(embeddings):\n",
        "        # Each vector is a tuple which contains the (id, values, metadata)\n",
        "        vectors.append((f'message-{i+1}', embedding, {'text': history[i]}))\n",
        "\n",
        "    # Upserting the vectors into Pinecone\n",
        "    index.upsert(vectors)\n",
        "add_embeddings_to_pinecone(history, index_name=PINECONE_INDEX_NAME)"
      ],
      "metadata": {
        "id": "GecFspgQf-V8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_history(query, history, index_name='chat-history'):\n",
        "    index = pc.Index(index_name)\n",
        "\n",
        "    # Encoding the query\n",
        "    query_embedding = genai.embed_content(model=\"models/text-embedding-004\", content=[query])['embedding'][0]\n",
        "\n",
        "    # Querying the Pinecone for relevant history\n",
        "    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
        "\n",
        "    # Just a small debugging step: Print the results to see if any matches are found\n",
        "    print(f\"Query results: {results}\")\n",
        "\n",
        "    # Retrieving the most relevant history messages\n",
        "    relevant_messages = [match['metadata']['text'] for match in results['matches'] if 'metadata' in match]\n",
        "\n",
        "    return relevant_messages\n",
        "\n"
      ],
      "metadata": {
        "id": "s--WLJc_lWsh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_prompt(test_prompt, history, index_name='chat-history'):\n",
        "    \"\"\"\n",
        "    This function:\n",
        "    1. Retrieves relevant history messages using the RAG mechanism.\n",
        "    2. Combines the retrieved messages with the test prompt.\n",
        "    3. Ensures the combined prompt does not exceed the maximum token limit.\n",
        "    4. Returns the combined prompt and the context referred.\n",
        "    \"\"\"\n",
        "    # Retrieving relevant history using the RAG mechanism\n",
        "    relevant_history = retrieve_relevant_history(test_prompt, history, index_name)\n",
        "\n",
        "    # Combining the retrieved history with the test prompt\n",
        "    combined_prompt = \"\\n\".join(relevant_history) + f\"\\nUser: {test_prompt}\\nAssistant:\"\n",
        "\n",
        "    # Let's assume our max token limit is 4096\n",
        "    if len(combined_prompt.split()) > 4096:\n",
        "        combined_prompt = ' '.join(combined_prompt.split()[:4096])\n",
        "\n",
        "    return combined_prompt, relevant_history\n",
        "\n"
      ],
      "metadata": {
        "id": "8Iku3cQ-l90P"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_final_prompt():\n",
        "    \"\"\"\n",
        "    This function:\n",
        "    1. Defines the final test prompt.\n",
        "    2. Prepares the prompt using the prepare_prompt function.\n",
        "    3. Calls OpenAI to generate a response.\n",
        "    4. Prints the final test prompt, the chat history context referred to, and the final response.\n",
        "    5. Validates manually if the final response references the correct context.\n",
        "    \"\"\"\n",
        "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
        "\n",
        "    # Preparing the prompt and retrieving the context\n",
        "    prepared_prompt, context_referred = prepare_prompt(final_test_prompt, history)\n",
        "\n",
        "    # Generating a response from the Gemini model\n",
        "    response = model.generate_content(contents=prepared_prompt)\n",
        "\n",
        "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
        "    print(f\"Context Referred: {context_referred}\")\n",
        "    print(f\"Final Test Prompt Response: {response.candidates[0].content.parts[0].text}\")\n",
        "\n",
        "\n",
        "test_final_prompt()\n"
      ],
      "metadata": {
        "id": "slC3sFnZm4CG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "98de2af9-74ae-402d-dcf8-af67589edbb6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query results: {'matches': [{'id': 'message-7',\n",
            "              'metadata': {'text': '7: User: Perfect! Now, what are some good '\n",
            "                                   'exercises? | Bot: You can try a mix of '\n",
            "                                   'cardio and strength training exercises.'},\n",
            "              'score': 0.471715927,\n",
            "              'values': []},\n",
            "             {'id': 'message-8',\n",
            "              'metadata': {'text': '8: User: Any specific recommendations for '\n",
            "                                   'cardio? | Bot: Running, cycling, and '\n",
            "                                   'swimming are all excellent cardio '\n",
            "                                   'exercises.'},\n",
            "              'score': 0.430772066,\n",
            "              'values': []},\n",
            "             {'id': 'message-9',\n",
            "              'metadata': {'text': \"9: User: I'll start with running. Can you \"\n",
            "                                   \"recommend any books? | Bot: 'Atomic \"\n",
            "                                   \"Habits' by James Clear is a highly \"\n",
            "                                   'recommended book.'},\n",
            "              'score': 0.410259396,\n",
            "              'values': []}],\n",
            " 'namespace': '',\n",
            " 'usage': {'read_units': 6}}\n",
            "Final Test Prompt: Do you think it will help me stay fit?\n",
            "Context Referred: ['7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.', '8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.', \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\"]\n",
            "Final Test Prompt Response: 'Atomic Habits' is a great book for building good habits in general, which can definitely help you stay fit. However, it's not specifically about running.  For running books, you might want to look into:\n",
            "\n",
            "* **\"Born to Run\" by Christopher McDougall:** This book explores the natural running style of the Tarahumara tribe and suggests ways to run more naturally and avoid injury.\n",
            "* **\"The Complete Guide to Running\" by Greg McMillan:** This comprehensive guide covers everything from training plans to nutrition and injury prevention.\n",
            "* **\"Run Less, Run Faster\" by Greg McMillan:** This book focuses on high-intensity training methods to improve your running speed and efficiency.\n",
            "\n",
            "Remember, the best book for you will depend on your specific goals and running experience. Happy running! \n",
            "\n"
          ]
        }
      ]
    }
  ]
}